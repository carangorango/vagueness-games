\documentclass[a4paper]{article}

\usepackage[natbib=true,style=authoryear-comp,backend=biber,doi=false,url=false]{biblatex}
\bibliography{paper}

\usepackage{todonotes}
\usepackage{a4wide}
\usepackage{amssymb}
\usepackage{syllogism}

\begin{document}

\title{Natural sources of vagueness and their implications}
\author{Michael Franke \and Jos\'e Pedro Correia}
\date{}

\maketitle

\begin{abstract}
A vexing puzzle about vagueness, rationality and evolution runs, in crude abbreviation, as follows: vague language use is demonstrably suboptimal if the goal is efficient precise and cooperative information transmission; hence rational deliberation or evolutionary selection should, under this assumed goal, eradicate vagueness from language use.
Since vagueness is persistent in all human languages, something has to give.
In this paper, we investigate a number of reasons why and mechanisms how vagueness may come into the picture in formal models of rational or evolutionary optimal signaling.
We show how uncertainty about not only the linguistic practices of others, but also about the world itself can lead to vagueness, and how, given vagueness, natural linguistic practices are likely to create more reason for vagueness.
We explore the consequences of these reasons and mechanism for a notion of meaning and a notion of language.
\end{abstract}

\tableofcontents

\section{Vagueness}
\label{sec:vagueness}

The classical philosophical problem of vagueness is most starkly embodied by the sorites paradox.
The original formulation is attributed to Eubulides, an ancient Megarian philosopher~\parencite{sorensen_sorites_2009}, and uses the example of a heap of sand: if one million grains of sand piled up together clearly form a heap, and removing one grain of sand can never make a heap into a non-heap, then by repeated applications of this reasoning we are lead to acknowledge that one single grain of sand also forms a heap.
This paradox can be stated in a very general way.
We start with an $x$ to which a certain predicate $P$ clearly applies.
We observe that a certain transformation $f$ has no impact on whether something is or is not $P$.
Thus we are lead to acknowledge that the predicate $P$ also applies to $y = f(x)$.
Moreover, we can apply the same reasoning subsequently to $f(x)$ and repeat that as many times as the transformation is still applicable.
The paradox arises when $n$ repeated applications of the transformation $f$ lead us from $x$ to $z = f^n(x)$, where $z$ is something for which $P$ clearly does not apply.
However, the original premisses seem to force us to accept that $P$ does apply to $x$.
The paradox with the heap can thus be seen as an instance of this general formulation, if $x$ is the group of grains of sand, the transformation is removing a grain, and $z$ the state of one or zero grains.
It can also be reversed: it seems reasonable to accept that piling up a grain of sand to something that is not a heap will not turn it into a heap of sand, thus 
by repeated applications of that transformation one can pile up as many grains of sand as one likes and will never obtain a heap.

Predicates for which one can find a suitable instance of the general formulation of the sorites paradox are called \emph{vague}.
Paradigmatic examples besides `heap' include `tall', `red', `bald', `tadpole', and `child'~\parencite{Keefe1997}.
An important intuition regarding these predicates that enables the sorites paradox to work is that, for a certain level of granularity, there is no clear boundary demarcating the cases where the predicate applies from the cases where it doesn't, but there are nevertheless examples of both cases.
Intuitively, a person with a height of 2 meters is tall, and one with a height of 1.5 meters is not, but there is no value of height in between where one would draw the line; saying, for example, that a person with a height of 1.80 meters is tall, but a person measuring 1.79 meters is not, just seems preposterous.
Neither drawing a sharp boundary seems reasonable, nor do the consequences of not doing so, and in there lies the truely paradoxical issue of vagueness.
How widespread is the problem?
Mereological nihilists argue that instances of the sorites can be designed for any material object that can be decomposed into small enough parts.
If we subscribe to the scientific picture of material objects as composed of molecules and atoms, this applies to tables and chairs, cats and mats, and any other ordinary thing~\parencite{Unger1979}.
Any heap of molecules that we have a name for is potentially at the mercy of being hypotheticall chipped away into nothingness while still keeping its name.
The problem thus seems to be a serious one.
In face of such a powerful paradox like the sorites, something has got to give.

Vagueness is typically seen as a challenge to a certain classical conception of language and meaning.
This is the picture that our words stand in a direct or mediated correspondence to their meanings; sentences are combinations of such words bound by logical rules; knowledge of the meanings of words and of how the rules combine them allows us to know the meaning of sentences and determine whether they are true or false.
With variations, this picture could be said to underlie theories of meaning of authors life Gottlob Frege, Bertrand Russell, Ludwig Wittgenstein (his early work), Alfred Tarski, Richard Montague, and many others, especially those working in the analytic tradition.
Vagueness can be seen as a challenge to such a view in different ways.
Vague predicates seem to lack precise boundaries; if that is the case, how can words like `tall' stand in correspondence to anything at all?
The classical idea that words like `table' stand in a relation to a set of entities for which it is true to say that they are tables is problematic in light of a sorites paradox that stresses the boundaries of such a hypothetical set of entities to show that there is no way of defining one.
Is this a problem of sticking to a bivalent notion of truth?
Can we solve the issue by adopting more truth values besides `true' and `false'?
What other alternatives are there?

\todo[inline]{Should we include the following detailed arguments?}
Some argue that we should stick to the classical picture outlined above.
Timothy Williamson, for example, claims that ``[c]lassical logic and semantics are vastly superior to the alternatives in simplicity, power, past success, and integration with theories in other domains.''~\parencite*[162]{williamson_vagueness_1992}
In the face of vagueness, Williamson defends the so-called \emph{epistemic view}: apparently vague terms do determine precise boundaries, we just don't have the ability to know where they are, \emph{i.e.}~vagueness is a kind of ignorance.
There is thus a precise height above which people are tall, and below which they are not, we just cannot know what that value is.
The epistemic approach is curious in that in its attempt to save truth, it renders it sterile as an explanatory notion.
If we can never ascertain truth when it comes to vague terms, how can truth be relevant for explaining how we understand or produce language?
If the concept of truth is crucially tied to meaning, but we have no access to the former, how could we ever have learned the latter?
Williamson's reason to ignore these problems is a reasonable one: he believes it helps us, as philosophers or linguists, to better understand language.
But unless we strongly agree with this assessment of the success of the classical picture, we would do better to search for a more intuitive alternative.

Another suggested theory that attempts to retain as much as possible from classical logic by making minimal modifications to the classical picture of semantics is \emph{supervaluationism}.
Championed by Kit Fine~\parencite*{Fine1975}, the idea is that vague predicates can be made precise, even arbitrarily.
Setting a boundary between tall and not tall people at 1.80 meters would be an admissible%
\footnote{Only precisifications that meet certain constraints should be considered. We refer the reader to Fine's article~\parencite*{Fine1975} for the details.}
\emph{precisification} of the word `tall'.
What supervaluationism defends is that, when considering the truth value of a sentence with vague terms, we should take all admissible precisifications of those terms into account: the sentence is true if true under all of them, false if false under all of them, and neither true nor false otherwise.
The intuition is that a person with a height of 2 meters would be considered tall under all admissible precisifications, thus we could say it is true that that person is tall; for someone measuring 1.79 this would not be the case, hypothetically being true under some precisifications and false under others.
%In defending this approach, supervaluationism either assumes or postulates the potential existence of a boundary between the sentences for which all admissible precisifications of the vague terms therein make them true, and those for which at least one precisification makes them false.
Although supervaluationism claims to retain classical logics, it seems to implicitly require a third truth value to account for sentences for which not all precisifications are in agreement.
Other responses to the problem of vagueness develop alternative logics that explicitly break away from the bivalent assumption that a sentence is either true or false, and introduce additional truth values.
These range from three-valued logics~\parencite[\emph{e.g.}][]{tye_sorites_1994} to infinite-valued degree theories~\parencite[\emph{e.g.}][]{machina_truth_1976}.

Mark Sainsbury~\parencite*{sainsbury_concepts_1999} argues that all of these solutions fail to address an important characteristic of vague predicates: \emph{higher order vagueness}.
Supervaluationism, many-valued logics, and degree theories all end up being committed to new artificial demarcating boundaries (\emph{e.g.}~true-under-all-precisifications versus neither true nor false versus false-under-all-precisifications, true versus indefinite versus false, true to degree 1 versus true to degree 0 versus the rest).
But a vague predicate not only fails to demarcate between the cases where it clearly applies and the ones where it clearly doesn't, it also fails to establish a boundary between the cases where it clearly applies and the borderline cases, as well as between the borderline cases and the cases where it clearly doesn't apply.
Further introducing borderline borderline cases would lead into an infinite regress.
The point that these classical approaches to vagueness miss is aptly described as follows:
\begin{quote}
But to what in our actual use of language does this division correspond?
It looks as if, as before, it should correspond to the sentences true beyond the shadow of vagueness, those in some kind of borderline position, and those false beyond the shadow.
But [\ldots] we do not know, cannot know, and do not need to know these supposed boundaries to use language correctly.
Hence they cannot be included in a correct description of our language.%
~\parencite[256]{sainsbury_concepts_1999}
\end{quote}
By trying to cling as much as possible to the classical picture of logic and semantics, these approaches are ignoring a simple observation: natural language users are sensitive to the sorites paradox, \emph{i.e.} are able to recognize the logical inconsistency but do not have a good answer to overcome it.
Even more importantly, we apparently do not need to solve the inconsistency in order to continue using natural language productively.
Nobody ever stopped using the word `tall` after being confronted with a sorites series to deconstruct it.
Why should we develop theories of meaning that are impervious to the paradox?
In attempting to do so, we are imposing a requirement that is contrary to our observations, and by refusing to give that requirement up we are producing unintuitve, unrealistic theories of meaning that, worst of all, do not even seem to resolve the paradox but merely obfuscate the problem.
Ludwig Wittgenstein put it better:
\begin{quote}
The more closely we examine actual language, the greater becomes the conflict between it and our requirement.
(For the crystalline purity of logic was, of course, not something I had \emph{discovered}: it was a requirement.)
The conflict becomes intolerable; the requirement is now in danger of becoming vacuous.
-- We have got on to slippery ice where there is no friction, and so, in a certain sense, the conditions are ideal; but also, just because of that, we are unable to walk.
We want to walk: so we need \emph{friction}.
Back to the rough ground!%
~\parencite[\S 107]{wittgenstein_philosophical_1953}
\end{quote}


\section{Paradigm shift}
\label{sec:Wittgenstein-and-signaling}

\section{Signaling models of vagueness}
\label{sec:vague-signaling}


\printbibliography

\end{document}
