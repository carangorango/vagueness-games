\documentclass[fleqn,reqno,10pt]{article}

%========================================
% Packages
%========================================

\usepackage[]{../../helpers/mypackages}
%\usepackage[natbib=true,style=authoryear-comp,backend=bibtex,doi=false,url=false]{biblatex}
%\bibliography{MyRefGlobal}
\bibliography{../../helpers/MyRefGlobal}
\bibliography{paper} 
\usepackage{../../helpers/myenvironments}
\usepackage{../../helpers/mycommands}
\usepackage{todonotes}
\usepackage{subcaption}



%========================================
% Standard Layout
%========================================

% Itemize
\renewcommand{\labelitemi}{\large{$\mathbf{\cdot}$}}    % itemize symbols
\renewcommand{\labelitemii}{\large{$\mathbf{\cdot}$}}
\renewcommand{\labelitemiii}{\large{$\mathbf{\cdot}$}}
\renewcommand{\labelitemiv}{\large{$\mathbf{\cdot}$}}
% Description
\renewcommand{\descriptionlabel}[1]{\hspace\labelsep\textsc{#1}}

% Figure Captions
\usepackage{caption} % use corresponding myfiguresize!
\setlength{\captionmargin}{20pt}
\renewcommand{\captionfont}{\small}
\setlength{\belowcaptionskip}{7pt} % standard is 0pt

%========================================
% Additional layout & commands
%========================================


\renewcommand{\Smixed}{\ensuremath{\mathrm{\mathbf{s}}}}
\renewcommand{\Rmixed}{\ensuremath{\mathrm{\mathbf{r}}}}

% Annotations
\newcommand{\mytodo}[2]{\todo[inline,color=yellow,author=#1]{#2}}
\newcommand{\question}[2]{\todo[inline,color=blue,author=#1]{#2}}
\newcommand{\answer}[2]{\todo[inline,color=green,author=#1]{#2}}

\newcommand{\rd}{\acro{rd}} % replicator dynamic
\newcommand{\rmd}{\acro{rmd}} % replicator mutator dynamic
\newcommand{\rdd}{\acro{rdd}} % replicator diffusion dynamic
\newcommand{\RD}{\ensuremath{\mathrm{RD}}} % replicator dynamic
\newcommand{\RDD}{\ensuremath{\mathrm{RDD}}} % replicator diffusion dynamic
\newcommand{\RMD}{\ensuremath{\mathrm{RMD}}} % replicator mutator
                                
\newcommand{\Diff}{\ensuremath{\mathrm{D}}} % Difusion 
\newcommand{\Mutate}{\ensuremath{\mathrm{M}}} % Mutation 

\newcommand{\impairment}{\ensuremath{\alpha}} % impairment
\newcommand{\toler}{\ensuremath{\beta}} % tolerance
\newcommand{\ns}{\ensuremath{n_s}} % number of states

\newcommand{\similarity}{\ensuremath{\mathrm{Sim}}} % similarity function

\doublespacing

\begin{document}

\section*{Derivation of the RDD by imitation-based revision}

We derive the proposed \rdd as a mean dynamic of an imitative revision
protocol. For expository reasons we follow closely the example 4.3.1
of \citet{Sandholm2010:Population-Game}, but other possibilities
exist. Our derivation relies on the usual assumptions about
conditional imitation at the agent level, but must add special
assumptions about how noise affects revision probabilities. In keeping
with the idea that agents are of possibly limited cognitive
capacities, we assume here that noise is real but that agents are
unaware of it. When agents perceive the actual expected utility of
their choice and of another agent's choice (as is usual in the
motivation of imitation-based revision protocols), we assume
additionally that agents mistake their possibly non-veridical
subjective representations of states for the actual ones (details
below).

\paragraph{Notation.} As previously in this paper, let
$\Sstrat(\mymessg{i} \probbar \mystate{o})$ be the probability that a
speaker randomly picked from the population chooses $\mymessg{i}$ if
the actual/objective state is $\mystate{o}$. This factors in the
possibility that the sampled speaker perceives the objective state
$\mystate{o}$ as some subjective state $\mystate{s}$. In contrast, let
$\Sstrat_s(\mymessg{i} \probbar \mystate{s})$ be the probability that
a randomly sampled speaker chooses $\mymessg{i}$ if she experiences
some objective state $\mystate{o}$ as $\mystate{s}$. This is then also
the average sender behavior in a (possibly counterfactual) world
without perceptual confusion. It is also where behavioral changes
first take effect, to then percolate to $\Sstrat$ as well.

Similarly, $\Rstrat(\mystate{o} \probbar \messg)$ is the probability
that a randomly sampled receiver chooses interpretation
$\mystate{o}$. $\Rstrat_s(\mystate{s} \probbar \messg)$ is the
probability of a subjectively intended interpretation
$\mystate{s}$. Again, behavioral changes first target $\Rstrat_s$ but
cause changes in $\Rstrat$ as well.

Let $\EU(\messg)$ be short for $\EU(\messg,\state,\Rstrat)$ whenever
$\state$ and $\Rstrat$ are implicit, similarly for $\EU(\state)$ as a
shorthand for $\EU(\state,\messg,\Sstrat)$.

\paragraph{Standard assumptions.} Let's assume that we are in an
entirely noise-free world so that $\Sstrat = \Sstrat_s$ and $\Rstrat =
\Rstrat_s$. The standard assumptions for imitation-based revisions are
these. Agents get revision opportunities for all actual choice points
with equal probability (independently of how likely these would occur
in actual play). Take the case of a sender (receiver side is
parallel). If agent $A$ is given a revision opportunity for actual
state $\state$, she consults her own choice, which is $\mymessg{i}$
with probability $\Sstrat(\mymessg{i} \probbar \state)$, and observes
the behavior of one randomly picked other agent $B$, which is
$\mymessg{j}$ with probability $\Sstrat(\mymessg{j} \probbar
\state)$. $A$ also observes the expected utility of both choices (at
$\state$) and switches to $B$'s choice with probability $r_{ij} = [
\EU(\mymessg{j}) - \EU(\mymessg{i}) ]_+$, so that the overall
switching probability of an agent like $A$ who samples $\mymessg{i}$
for $\state$ is $p_{ij} = \Sstrat(\mymessg{j} \probbar \state) \
r_{ij}$.

\paragraph{Imitation under perceptual noise.} There are many
conceivable ways in which perceptual noise might influence the above
revision protocol. Our choice here is guided by theoretical and
practical considerations alike. We would like maintain the common
assumption that agents are ideal optimizers and not necessarily fully
aware of their surroundings. In particular, we assume here that agents
are not aware of perceptual noise. Switching probabilities are
therefore defined in terms of actual expected utility (and not
subjective expectations that take noise into consideration). Moreover,
we assume that agents who have (unconscious) access to expected
utilities for actual states $\mystate{o}$ will confuse whatever
subjective representation $\mystate{s}$ they may have had during their
revision opportunity for the actual state $\mystate{o}$.

Concretely, suppose agent $A$ has a revision opportunity for objective
state $\mystate{o}$, and chooses $\mymessg{i}$ with probability
$\Sstrat(\mymessg{i} \probbar \mystate{o})$. Similarly, $B$ chooses
$\mymessg{j}$ with probability $\Sstrat(\mymessg{j} \probbar
\mystate{o})$. It may be that $A$ perceived $\mystate{o}$ as
$\mystate{s_A}$ and that $B$ perceived $\mystate{s_B}$, but since $A$
is unaware of noise he expects no difference. $A$ observes (as usual)
the actual expected utilities of $\mymessg{i}$ and $\mymessg{j}$ for
$\mystate{o}$. Consequently, he updates his subjective behavior for
$\mystate{o}$ (not being aware that the perceived $\mystate{s_A}$
might have been different). This myopic revision behavior gives rise
to expected changes in the subjective/noise-free choice probabilities
$\Sstrat_s(\mymessg{i} \probbar \state)$ as follows:
\begin{align*}
  \dot{\Sstrat}_s(\mymessg{i} \probbar \state) & = \sum_j
  \Sstrat(\mymessg{j} \probbar \state) \cdot p_{ji} -  
  \Sstrat(\mymessg{i} \probbar \state) \cdot \sum_j p_{ij} \\
  & = \sum_j  \Sstrat(\mymessg{j} \probbar \state) \cdot
    \Sstrat(\mymessg{i} \probbar \state) \cdot [\EU(\mymessg{i}) -
    \EU(\mymessg{j})]_+ \\ 
   & \ \ \ \  - \Sstrat(\mymessg{i} \probbar \state) \cdot \sum_j
   \Sstrat(\mymessg{j} \probbar \state) \cdot [\EU(\mymessg{j}) -
    \EU(\mymessg{i})]_+ \\
   & = \Sstrat(\mymessg{i} \probbar \state) \cdot \left ( \sum_j
     \Sstrat(\mymessg{j} \probbar \state) \cdot \EU(\mymessg{i}) - \sum_j
     \Sstrat(\mymessg{j} \probbar \state) \cdot \EU(\mymessg{j})
   \right ) \\
   & = \Sstrat(\mymessg{i} \probbar \state) \cdot \left (
     \EU(\mymessg{i}) - \text{average EU at $\state$} \right )
\end{align*}
This is the (continuous-time) replicator dynamics for behavioral
strategies.

Similar considerations motivate the receiver part. Suppose that agent
$A$ has a revision opportunity for choice point $\messg$. He and a
random other agent $B$ actually choose $\mystate{o_A}$ and
$\mystate{o_B}$ with probability $\Rstrat(\mystate{o_{A,B}} \probbar
\messg)$, even though they may have intended interpretations
$\mystate{s_A}$ and $\mystate{s_B}$. Agent $A$ observes the actual
expected utility and, by assumption, believes (myopically) that he had
wanted to realize $\mystate{o_A}$ and that $B$ had wanted to realize
$\mystate{o_B}$. Under these assumptions, the derivation of the
replicator dynamic for subjectively intended (noise-free) receiver
behavior $\Rstrat_s$ proceeds in parallel with the above sender case.

At each time step, the noise-free behavior of agents is, of course,
perturbed by noise. This is why, after each update step, the \rdd
defined in Equation~XYZ perturbes the outcome of the replicator step
with perceptual noise.



\printbibliography[heading=bibintoc]

\end{document}
